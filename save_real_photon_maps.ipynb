{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dca5390-b359-410c-b21d-00fcf77a20f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from glob import glob\n",
    "import re\n",
    "import h5py\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "def convert_to_real_photon_maps(photon_maps_old, gain=0.2):\n",
    "    \"\"\"\n",
    "    During the Lingjia/Zoey meeting on 10/18/2024 we noticed that the low q \n",
    "    photon intensity was too. After further investigating, it appears that \n",
    "    the photon maps saved in the folder: scratch/LS/Reduced_Data/reduced_3-17-23/\n",
    "    requires an additional gain multiplication and rounding to be converted to \n",
    "    actual photon arrival events. All data processed before 10/18/2024 using \n",
    "    the photon statistics method should not be trusted.\n",
    "    \n",
    "    Convert the \"*_photon.npy\" data that was saved in the reduced_3-17-23 folder\n",
    "    to (hopefully) real photon maps with discrete values.\n",
    "\n",
    "    Parameters:\n",
    "    photon_maps_old (np.ndarray): The old photon maps.\n",
    "    gain (float, optional): The gain factor to apply to the photon data. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "    discrete_photon (np.ndarray): The discrete photon map with values in the range [0, 255].\n",
    "\n",
    "    Notes:\n",
    "    The function loads photon data from a numpy file, applies a gain factor, and then \n",
    "    discretizes the values to the range [0, 255].\n",
    "    \"\"\"\n",
    "    photon = np.minimum(photon_maps_old, 255)\n",
    "    photon = np.maximum(photon, 0)\n",
    "    photon = np.rint(gain*photon).astype(np.uint8)\n",
    "    return photon\n",
    "\n",
    "\n",
    "def write_photon_map_h5_output_path(file_list, chunk_size, run, module):\n",
    "    \"\"\"\n",
    "    Create an HDF5 file for storing photon maps and initialize its metadata.\n",
    "\n",
    "    Parameters:\n",
    "    file_list (list): List of files containing photon data.\n",
    "    chunk_size (tuple): Chunk size for the HDF5 dataset.\n",
    "    run (int): Run number of the experiment.\n",
    "    module (int): Module number of the detector.\n",
    "    \"\"\"\n",
    "    # Define the output file path\n",
    "    output_file = f'./data/photon_maps/run{run:03d}_module{module:02d}.h5'\n",
    "\n",
    "    # Get the shape of the last file in the list\n",
    "    last_file_shape = np.load(file_list[-1], 'r').shape\n",
    "\n",
    "    # Calculate the total number of trains\n",
    "    n_trains = ((len(file_list)-1)*300) + last_file_shape[0]\n",
    "\n",
    "    # Define the new shape for the HDF5 dataset\n",
    "    new_shape = (n_trains, *last_file_shape[-3:])\n",
    "\n",
    "    # Experimental parameters for saving metadata\n",
    "    current = [0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]\n",
    "    dark_runNB = [69,  72,  72,  75,  75,  78,  78,  81,  81]\n",
    "    runNB =      [70,  71,  73,  74,  76,  77,  79,  80,  82]\n",
    "    index = runNB.index(run)\n",
    "\n",
    "    # Create the HDF5 file and initialize its metadata\n",
    "    with h5py.File(output_file, 'w') as f:\n",
    "        # Create the dataset for storing photon maps\n",
    "        f.create_dataset('photon_maps', new_shape, chunks=chunk_size, dtype=np.uint8)\n",
    "\n",
    "        # Set the metadata attributes\n",
    "        f.attrs['run'] = run\n",
    "        f.attrs['dark_run'] = dark_runNB[index]\n",
    "        f.attrs['module'] = module\n",
    "        f.attrs['proposal'] = 2884\n",
    "        f.attrs['field'] = current[index]\n",
    "        f.attrs['date_processed'] = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def update_photon_maps(run, module, chunk_size=(10, 200, 128, 512), overwrite_file=False):\n",
    "    \"\"\"\n",
    "    During the Lingjia/Zoey meeting on 10/18/2024 we noticed that the low q \n",
    "    photon intensity was too. After further investigating, it appears that \n",
    "    the photon maps saved in the folder: scratch/LS/Reduced_Data/reduced_3-17-23/\n",
    "    requires an additional gain multiplication and rounding to be converted to \n",
    "    actual photon arrival events. All data processed before 10/18/2024 using \n",
    "    the photon statistics method should not be trusted.\n",
    "    \n",
    "    Converts the \"*_photon.npy\" data that was saved in the reduced_3-17-23 folder\n",
    "    to (hopefully) real photon maps with discrete values in HDF5 format.\n",
    "\n",
    "    Parameters:\n",
    "    run (int): Run number of the experiment.\n",
    "    module (int): Module number of the detector.\n",
    "    chunk_size (tuple, optional): Chunk size for the HDF5 dataset. Defaults to (2000, 128, 512).\n",
    "    overwrite_file (bool, optional): Whether to overwrite the existing file. Defaults to False.\n",
    "    \"\"\"\n",
    "    # Define the folder path containing the photon data files\n",
    "    folder = f'/gpfs/exfel/u/scratch/SCS/202201/p002884/LS/Reduced_Data/reduced_3-17-23/'\n",
    "\n",
    "    # Get the list of files containing photon data\n",
    "    files = sorted(glob(folder+f'r{run:0d}m{module:0d}_*_photon.npy'), \n",
    "                   key=lambda s: int(re.findall(r'\\d+', s)[-1]))\n",
    "\n",
    "    # Check if the output file already exists\n",
    "    if not overwrite_file and os.path.exists(output_file):\n",
    "        print('Output file already exists.')\n",
    "    else:\n",
    "        # Create the HDF5 file and initialize its metadata\n",
    "        write_photon_map_h5_output_path(files, chunk_size, run, module)\n",
    "\n",
    "    # Iterate over the files and update the photon maps in the HDF5 file\n",
    "    for i, file in tqdm(enumerate(files), total=len(files)):\n",
    "        # Load the photon data from the file\n",
    "        data = da.from_array(np.load(file, 'r'))\n",
    "\n",
    "        # Rechunk the data to match the chunk size of the HDF5 dataset\n",
    "        data = data.rechunk(chunks=(10, -1, -1, -1))\n",
    "\n",
    "        # Convert the photon data to real photon maps\n",
    "        photon_maps = convert_to_real_photon_maps(data)\n",
    "        \n",
    "        photon_maps = photon_maps.reshape((-1, *photon_maps.shape[-3:]))\n",
    "        photon_maps = photon_maps.compute()\n",
    "\n",
    "        # Calculate the start and stop indices for storing the photon maps\n",
    "        start_index = i*(300)\n",
    "        stop_index = start_index + (data.shape[0])\n",
    "\n",
    "        # Define the output file path\n",
    "        output_file = f'./data/photon_maps/run{run:03d}_module{module:02d}.h5'\n",
    "        # Store the photon maps in the HDF5 file\n",
    "        with h5py.File(output_file, 'a') as f:\n",
    "            f['photon_maps'][start_index:stop_index] = photon_maps \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62edb5-3ed3-4ba6-90bc-d6632a8be321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4f83d9dc6944c2b7e5841158a9be8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update_photon_maps(run=70, module=0, overwrite_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61236a8-964e-4fc6-805c-1bc2e94dbee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SCS Toolbox (p002884)",
   "language": "python",
   "name": "toolbox_p002884"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
